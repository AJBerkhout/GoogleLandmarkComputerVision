{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f90653c20065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "  #!/usr/bin/env python\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import keras\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "\n",
    "\n",
    "IN_KERNEL = os.environ.get('KAGGLE_WORKING_DIR') is not None\n",
    "MIN_SAMPLES_PER_CLASS = 20\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_STEP = 3\n",
    "LR_FACTOR = 0.5\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "MAX_STEPS_PER_EPOCH = 15000\n",
    "NUM_EPOCHS = 1 #2 ** 32\n",
    "LOG_FREQ = 500\n",
    "NUM_TOP_PREDICTS = 20\n",
    "TIME_LIMIT = 9 * 60 * 60\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, mode: str) -> None:\n",
    "        print(f'creating data loader - {mode}')\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "\n",
    "        self.df = dataframe\n",
    "        self.mode = mode\n",
    "\n",
    "        transforms_list = []\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            transforms_list = [\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomChoice([\n",
    "                    transforms.RandomResizedCrop(64),\n",
    "                    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "                    transforms.RandomAffine(degrees=15, translate=(0.2, 0.2),\n",
    "                                            scale=(0.8, 1.2), shear=15,\n",
    "                                            resample=Image.BILINEAR)\n",
    "                ])\n",
    "            ]\n",
    "\n",
    "        transforms_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.transforms = transforms.Compose(transforms_list)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        ''' Returns: tuple (sample, target) '''\n",
    "        filename = self.df.id.values[index]\n",
    "\n",
    "        part = 1 if self.mode == 'test' or filename[0] in '01234567' else 2\n",
    "        directory = 'test' if self.mode == 'test' else 'train_' + filename[0]\n",
    "        sample = Image.open(f'../input/google-landmarks-2019-64x64-part{part}/{directory}/{self.mode}_64/{filename}.jpg')\n",
    "        assert sample.mode == 'RGB'\n",
    "\n",
    "        image = self.transforms(sample)\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return image\n",
    "        else:\n",
    "            return image, self.df.landmark_id.values[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def GAP(predicts: torch.Tensor, confs: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    ''' Simplified GAP@1 metric: only one prediction per sample is supported '''\n",
    "    assert len(predicts.shape) == 1\n",
    "    assert len(confs.shape) == 1\n",
    "    assert len(targets.shape) == 1\n",
    "    assert predicts.shape == confs.shape and confs.shape == targets.shape\n",
    "\n",
    "    _, indices = torch.sort(confs, descending=True)\n",
    "\n",
    "    confs = confs.cpu().numpy()\n",
    "    predicts = predicts[indices].cpu().numpy()\n",
    "    targets = targets[indices].cpu().numpy()\n",
    "\n",
    "    res, true_pos = 0.0, 0\n",
    "\n",
    "    for i, (c, p, t) in enumerate(zip(confs, predicts, targets)):\n",
    "        rel = int(p == t)\n",
    "        true_pos += rel\n",
    "\n",
    "        res += true_pos / (i + 1) * rel\n",
    "\n",
    "    res /= targets.shape[0] # FIXME: incorrect, not all test images depict landmarks\n",
    "    return res\n",
    "\n",
    "class AverageMeter:\n",
    "    ''' Computes and stores the average and current value '''\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def nearest_neighbor(train_data: Any, labels: Any):\n",
    "    n_neighbors = 15\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    clf.fit(train_data, labels)\n",
    "    clf\n",
    "\n",
    "def load_data() -> 'Tuple[DataLoader[np.ndarray], DataLoader[np.ndarray], LabelEncoder, int]':\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # only use classes which have at least MIN_SAMPLES_PER_CLASS samples\n",
    "    print('loading data...')\n",
    "    df = pd.read_csv('../input/google-landmarks-2019-64x64-part1/train.csv')\n",
    "    df.drop(columns='url', inplace=True)\n",
    "\n",
    "    counts = df.landmark_id.value_counts()\n",
    "    selected_classes = counts[counts >= MIN_SAMPLES_PER_CLASS].index\n",
    "    num_classes = selected_classes.shape[0]\n",
    "    print('classes with at least N samples:', num_classes)\n",
    "\n",
    "    train_df = df.loc[df.landmark_id.isin(selected_classes)].copy()\n",
    "    print('train_df', train_df.shape)\n",
    "    test_df = pd.read_csv('../input/google-landmarks-2019-64x64-part1/test.csv', dtype=str)\n",
    "    test_df.drop(columns='url', inplace=True)\n",
    "    print('test_df', test_df.shape)\n",
    "\n",
    "    # filter non-existing test images\n",
    "    exists = lambda img: os.path.exists(f'../input/google-landmarks-2019-64x64-part1/test/test_64/{img}.jpg')\n",
    "    test_df = test_df.loc[test_df.id.apply(exists)].copy()\n",
    "    print('test_df after filtering', test_df.shape)\n",
    "    assert test_df.shape[0] > 112000\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_df.landmark_id.values)\n",
    "    print('found classes', len(label_encoder.classes_))\n",
    "    assert len(label_encoder.classes_) == num_classes\n",
    "\n",
    "    train_df.landmark_id = label_encoder.transform(train_df.landmark_id)\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, mode='train')\n",
    "    test_dataset = ImageDataset(test_df, mode='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, num_workers=NUM_WORKERS, drop_last=True)\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    return train_loader, test_loader, label_encoder, num_classes\n",
    "\n",
    "def train(train_loader: Any, model: Any, criterion: Any, optimizer: Any,\n",
    "          epoch: int, lr_scheduler: Any) -> None:\n",
    "    print(f'epoch {epoch}')\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    avg_score = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    num_steps = min(len(train_loader), MAX_STEPS_PER_EPOCH)\n",
    "\n",
    "    print(f'total batches: {num_steps}')\n",
    "\n",
    "    end = time.time()\n",
    "    lr_str = ''\n",
    "    for i, (input_, target) in enumerate(train_loader):\n",
    "        if i >= num_steps:\n",
    "            break\n",
    "\n",
    "        output = model(input_.cuda())\n",
    "        loss = criterion(output, target.cuda())\n",
    "\n",
    "        confs, predicts = torch.max(output.detach(), dim=1)\n",
    "        avg_score.update(GAP(predicts, confs, target))\n",
    "\n",
    "        losses.update(loss.data.item(), input_.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % LOG_FREQ == 0:\n",
    "            print(f'{epoch} [{i}/{num_steps}]\\t'\n",
    "                        f'time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                        f'loss {losses.val:.4f} ({losses.avg:.4f})\\t'\n",
    "                        f'GAP {avg_score.val:.4f} ({avg_score.avg:.4f})'\n",
    "                        + lr_str)\n",
    "\n",
    "        if has_time_run_out():\n",
    "            break\n",
    "\n",
    "    print(f' * average GAP on train {avg_score.avg:.4f}')\n",
    "\n",
    "def inference(knn: Any, data_loader: Any, model: Any) -> Tuple[torch.Tensor, torch.Tensor,\n",
    "                                                     Optional[torch.Tensor]]:\n",
    "    ''' Returns predictions and targets, if any. '''\n",
    "    model.eval()\n",
    "\n",
    "    activation = nn.Softmax(dim=1)\n",
    "    all_predicts, all_confs, all_targets = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(data_loader, disable=IN_KERNEL)):\n",
    "            if data_loader.dataset.mode != 'test':\n",
    "                input_, target = data\n",
    "            else:\n",
    "                input_, target = data, None\n",
    "            nnPred = knn.predict(input_)\n",
    "            output = model(input_.cuda())\n",
    "            printf(output)\n",
    "            output = activation(output)\n",
    "        \n",
    "            confs, predicts = torch.topk(output, NUM_TOP_PREDICTS)\n",
    "            all_confs.append(confs)\n",
    "            all_predicts.append(predicts)\n",
    "\n",
    "            if target is not None:\n",
    "                all_targets.append(target)\n",
    "\n",
    "    predicts = torch.cat(all_predicts)\n",
    "    confs = torch.cat(all_confs)\n",
    "    targets = torch.cat(all_targets) if len(all_targets) else None\n",
    "\n",
    "    return predicts, confs, targets\n",
    "\n",
    "def generate_submission(train_loader: Any, test_loader: Any, model: Any, label_encoder: Any) -> np.ndarray:\n",
    "    sample_sub = pd.read_csv('../input/landmark-recognition-2019/recognition_sample_submission.csv')\n",
    "    train = [input_ for i, (input_, target) in enumerate(tqdm(data_loader, disable=IN_KERNEL)) ]\n",
    "    labels = [target for i, (input_, target) in enumerate(tqdm(data_loader, disable=IN_KERNEL)) ]\n",
    "    nn = nearest_neighbor(train, labels)\n",
    "    predicts_gpu, confs_gpu, _ = inference(nn, test_loader, model)\n",
    "    predicts, confs = predicts_gpu.cpu().numpy(), confs_gpu.cpu().numpy()\n",
    "\n",
    "    labels = [label_encoder.inverse_transform(pred) for pred in predicts]\n",
    "    print('labels')\n",
    "    print(np.array(labels))\n",
    "    print('confs')\n",
    "    print(np.array(confs))\n",
    "\n",
    "    sub = test_loader.dataset.df\n",
    "    def concat(label: np.ndarray, conf: np.ndarray) -> str:\n",
    "        return ' '.join([f'{L} {c}' for L, c in zip(label, conf)])\n",
    "    sub['landmarks'] = [concat(label, conf) for label, conf in zip(labels, confs)]\n",
    "\n",
    "    sample_sub = sample_sub.set_index('id')\n",
    "    sub = sub.set_index('id')\n",
    "    sample_sub.update(sub)\n",
    "\n",
    "    sample_sub.to_csv('submission.csv')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def has_time_run_out() -> bool:\n",
    "    return time.time() - global_start_time > TIME_LIMIT - 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    global_start_time = time.time()\n",
    "    train_loader, test_loader, label_encoder, num_classes = load_data()\n",
    "\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP,\n",
    "                                                   gamma=LR_FACTOR)\n",
    "    \n",
    "    #comment out this train block if loading an existing model\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print('-' * 50)\n",
    "        train(train_loader, model, criterion, optimizer, epoch, lr_scheduler)\n",
    "        lr_scheduler.step()\n",
    "        if has_time_run_out():\n",
    "            break\n",
    " \n",
    "    # ... after training, save your model \n",
    "    torch.save(model,'../input/mytraining.pt')\n",
    "\n",
    "    # .. to load your previously training model:\n",
    "    #model = torch.load('../output/kaggle/working/mytraining.pt')\n",
    "    generate_submission(train_loader, test_loader, model, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
